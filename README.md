<h1 align="center">AI Assignment Grader</h1>

An advanced AI-powered system for automated assignment grading, combining Large Language Models (LLMs), Model Context Protocol (MCP) for structured evaluation, and Plagiarism Detection for academic integrity.

## âœ¨ Key Features
âœ… **LLM-Powered Grading** â€“ Evaluates essays, code, and free-text responses using GPT-4, LLaMA, or Mistral.<br>
âœ… **Model Context Protocol (MCP)** â€“ Ensures structured, context-aware assessment with rule-based consistency.<br>
âœ… **Plagiarism Detection** â€“ Integrates with Turnitin or uses text similarity (e.g., cosine similarity, fingerprinting).<br>
âœ… **Dynamic Rubrics** â€“ Customizable grading criteria aligned with course objectives.<br>
âœ… **Bias Mitigation** â€“ Fairness checks to reduce subjective discrepancies.<br>
âœ… **Scalable Deployment** â€“ Supports cloud (AWS/GCP) and local (Docker) setups.<br>


## ðŸ›  Tech Stack

| COMPONENT	    | TECHNOLOGY
|---------------|----------------
| Backend	      | Python (FastAPI/Flask)
| AI Models	    | OpenAI GPT-4, LLaMA-3, Mistral (via Hugging Face)
| MCP Engine	  | Custom rule-based + LLM context blending
| Plagiarism	  | Turnitin API / DiffLib / NLP-based similarity
| Database	    |PostgreSQL / Firebase
| Frontend	    | Streamlit (Teacher Dashboard)


## ðŸš€ Installation
### Prerequisites

 - Python 3.10+
 - OpenAI API Key (or Hugging Face token)
 - (Optional) Turnitin API Key

### Setup
1.  Clone the repo:

```bash
git clone https://github.com/yourusername/ai-assignment-grader.git  
cd ai-assignment-grader
``` 
2.  Install dependencies:
```bash
pip install -r requirements.txt
```

3.  Configure `.env`:
```env
OPENAI_API_KEY="your_key"  
MCP_THRESHOLD=0.75  # Model Context Protocol confidence threshold  
PLAGIARISM_MODE="local"  # "turnitin" or "local"
```
4.  Run the API:
```bash
uvicorn app.main:app --reload  
```

## ðŸ“Œ Usage Examples
### 1. Grading via MCP + LLM
```python
from app.mcp.rule_engine import MCPGrader  
from app.llm.feedback import generate_feedback  

# Initialize MCP with subject-specific rules  
grader = MCPGrader(rubric="computer_science")  
score = grader.evaluate(submission_text, llm_assist=True)  

# Generate feedback  
feedback = generate_feedback(submission_text, score)  
```
